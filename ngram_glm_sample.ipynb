{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQ0m3Nndx9Ntg786airqFf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daoc-info/using_llms/blob/main/ngram_glm_sample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a simple generative language model based on N-grams (trigrams)\n",
        "\n",
        "First we get all the modules, libraries and objects that we need to work with:\n",
        "- *NLTK* is the Natural Language Tool Kit module\n",
        "- *movie_reviews* is a dataset from IMDb. We will use this data to train our model\n",
        "- a *defaultdic* is a dictionary that gives back a default value if the key does not exist. The default value is given with a lambda function. Our model will be a defaultdic.\n"
      ],
      "metadata": {
        "id": "Xk4_l8sh-ROr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vl6AbUBdwU9t",
        "outputId": "b240f566-90cf-4518-8a3c-0577770e35e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Package movie_reviews is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('movie_reviews')\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk import trigrams\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our **model** will be a dictionary, where the key is a tuple with the first two words\n",
        "in the trigram, and the value is another dictionary. If the key does not exist, it will be created with an empty dictionary as value. This inner\n",
        "dictionary has as key the third word in the trigram, and as value the probability for that word. If the key does not exist, it will be created with value 0"
      ],
      "metadata": {
        "id": "ZwIu3R819F3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = defaultdict(lambda: defaultdict(lambda: 0))"
      ],
      "metadata": {
        "id": "JjOM_dNMwqk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we count every trigram in every sentence in the dataset: we get the frequency"
      ],
      "metadata": {
        "id": "vSpfZDC_CVYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in movie_reviews.sents():\n",
        "    for w1, w2, w3 in trigrams(sentence):\n",
        "        model[(w1, w2)][w3] += 1"
      ],
      "metadata": {
        "id": "zaoJ5mn6CRFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we transform the frequencies into probabilities. The meaning is: given words 1 and 2, what is the probability of having word 3."
      ],
      "metadata": {
        "id": "xWi2SXyNCqdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for w1_w2 in model:\n",
        "    total_count = float(sum(model[w1_w2].values()))\n",
        "    for w3 in model[w1_w2]:\n",
        "        model[w1_w2][w3] /= total_count"
      ],
      "metadata": {
        "id": "Sj6ToHfwCodl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try now with the words: \"the fantastic\". The highest probability is in \"special\", so, the prediction should be: \"the fantastic special\""
      ],
      "metadata": {
        "id": "mG8w0kOdDRga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model[\"the\", \"fantastic\"].items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7eYO1k4yv0u",
        "outputId": "7bdfda13-42a5-4028-abb5-047daa3f4a71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([('preview', 0.08333333333333333), ('jon', 0.08333333333333333), ('animation', 0.08333333333333333), ('special', 0.16666666666666666), (',', 0.08333333333333333), ('four', 0.08333333333333333), ('back', 0.08333333333333333), ('concept', 0.08333333333333333), ('\"', 0.08333333333333333), ('work', 0.08333333333333333), ('through', 0.08333333333333333)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now, let's generate a longer sentence, let's say 10 words in total:\n",
        "- first, we create a list with the first two words\n",
        "- then, we loop 8 times, to complete 10 words. In each iteration:\n",
        "  - we get the last two words in the list as a tuple (our model has two words tuples as keys in the outter dictionary)\n",
        "  -then, we get the elements (items) in the inner dictionary, and we sort them, from higher to lower `reverse=True`, by its value `key=lambda x:x[1]` (value, or probability, is the second element, or index 1, on every item. The key, or word, is the first element, or index 0)\n",
        "  - finally (inside the loop), we get the word with the highest probability, and add it to the list of words\n",
        "- the very last thing is to print the list of words as a string, concatenating every element separated by a space, having: \"the fantastic special effects , and the film , and\""
      ],
      "metadata": {
        "id": "Pu_iiEQXbANZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_sentence = \"the fantastic\".split()\n",
        "\n",
        "for i in range(2, 10):\n",
        "  w1_w2 = tuple(new_sentence[-2:])\n",
        "  highest_probability = sorted(model[w1_w2].items(), key=lambda x:x[1], reverse=True)\n",
        "  new_sentence.append(highest_probability[0][0])\n",
        "\n",
        "print(\" \".join(new_sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6BNBFuBbMCm",
        "outputId": "120e8b3f-ca4d-4143-bd73-d6af30d68604"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the fantastic special effects , and the film , and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try with different words, so you can see how different sentences get generated. You will realize, also, that the generation of sentences is deterministic (you always get the same sentence when you start with the same two words). That's because we always take the more probable value. You can add some degree of randomness when choosing the next word and see how sentences start varying from one run to the other. That is, in principle, how some real world models work. Do it as an exercise, and try with different degrees of randomness. You will see how the model changes from more precise, to more creative or hallucinatory!\n",
        "\n",
        "One last thing to notice is that in this model we are considering that punctuation signs like the comma are also a token, that's why you see them in the final sentence. That is finally a design choice. You could easily get read of all punctuation signs before building your model. Nevertheless, you should agree that, at list in the example sentence, the commas are rather well placed!"
      ],
      "metadata": {
        "id": "Ym76O4c4TjeD"
      }
    }
  ]
}